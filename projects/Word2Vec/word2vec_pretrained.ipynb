{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec using NLTK/Glove/GoogleNews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from IPython.display import Image # To view image from location/url\n",
    "import keras\n",
    "import nltk\n",
    "import logging\n",
    "import multiprocessing\n",
    "import re\n",
    "import os\n",
    "import gensim\n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from nltk.corpus import brown, movie_reviews, treebank\n",
    "#nltk.download('brown')\n",
    "#nltk.download('movie_reviews')\n",
    "#nltk.download('treebank')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Input & Output files path\n",
    "inputDir = \"./data/\"\n",
    "outputDir = \"./data/outDir/\"\n",
    "inputDir_BigFiles = \"E:/BigFiles/\"\n",
    "outputDir_Bigfiles = \"E:/BigFiles/outDir/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load NLTK model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown corpus model saved at ./data/outDir/\n",
      "Size of file :19197.8 KB\n"
     ]
    }
   ],
   "source": [
    "brown_model = Word2Vec(brown.sents())\n",
    "movie_reviews_model = Word2Vec(movie_reviews.sents())\n",
    "treebank_model = Word2Vec(treebank.sents())\n",
    "brown_model.save(outputDir+\"brown_model\")\n",
    "size = str(round((os.path.getsize(outputDir+'brown_model')/1000),1))\n",
    "print('Brown corpus model saved at '+outputDir+ '\\nSize of file :'+size+ ' KB' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test NLTK model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('care', 0.9140259027481079), ('chance', 0.9021503925323486), ('job', 0.8922946453094482), ('trouble', 0.8632405996322632), ('easy', 0.860753059387207)]\n",
      "cereal\n"
     ]
    }
   ],
   "source": [
    "# Brown\n",
    "print(brown_model.most_similar('money', topn=5))\n",
    "#find the odd one out\n",
    "print (brown_model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n",
    "#vector representation of word human\n",
    "#print (brown_model[\"human\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('him', 0.7941465377807617), ('someone', 0.7558401823043823), ('sleep', 0.7459496259689331), ('attention', 0.7414337396621704), ('brain', 0.737573504447937)]\n",
      "movie\n"
     ]
    }
   ],
   "source": [
    "# Movie Reviews\n",
    "print(movie_reviews_model.most_similar('money', topn=5))\n",
    "#find the odd one out\n",
    "print (movie_reviews_model.doesnt_match(\"girl boy her movie\".split()))\n",
    "#vector representation of word human\n",
    "#print (movie_reviews_model[\"human\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('all', 0.9999011158943176), ('only', 0.9998963475227356), ('new', 0.9998949766159058), ('some', 0.999890923500061), (\"'\", 0.9998877048492432)]\n",
      "money\n"
     ]
    }
   ],
   "source": [
    "# Treebank\n",
    "print(treebank_model.most_similar('money', topn=5))\n",
    "#find the odd one out\n",
    "print (treebank_model.doesnt_match(\"financial money earning fat\".split()))\n",
    "#vector representation of word human\n",
    "#print (treebank_model[\"human\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove model size :171.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Convert glove_input_file in GloVe format into word2vec_output_file in word2vec format.\n",
    "gensim.scripts.glove2word2vec.glove2word2vec(glove_input_file = inputDir_BigFiles+\"glove.6B/glove.6B.50d.txt\", \n",
    "                                             word2vec_output_file = outputDir_Bigfiles+\"word2vec_glove_file\")\n",
    "size = str(round((os.path.getsize(outputDir_Bigfiles+\"word2vec_glove_file\")/1000000),1))\n",
    "print('Glove model size :'+size+ ' MB')\n",
    "# Load GloVe: Global Vectors for Word Representation Word2Vec model.\n",
    "glove_model = word2vec.KeyedVectors.load_word2vec_format(outputDir_Bigfiles+\"word2vec_glove_file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ronaldinho', 0.9371346235275269), ('rivaldo', 0.9020113348960876), ('ronaldo', 0.8994787931442261), ('figo', 0.8895716667175293), (\"eto'o\", 0.8803616166114807)]\n",
      "lunch\n"
     ]
    }
   ],
   "source": [
    "print(glove_model.most_similar('messi', topn=5))\n",
    "#find the odd one out\n",
    "print (glove_model.doesnt_match(\"cricket football sports lunch\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Google Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google model size :3644.3 MB\n"
     ]
    }
   ],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "google_model = word2vec.KeyedVectors.load_word2vec_format \\\n",
    "(inputDir_BigFiles+'GoogleNews_w2v/GoogleNews-vectors-negative300.bin', binary=True) \n",
    "size = str(round((os.path.getsize(inputDir_BigFiles+'GoogleNews_w2v/GoogleNews-vectors-negative300.bin')/1000000),1))\n",
    "print('Google model size :'+size+ ' MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Google Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('artificial', 0.6219794154167175), ('artifical', 0.5527142882347107), ('Artifical', 0.5464799404144287), ('Plastic', 0.5309567451477051), ('Synthetic', 0.47686296701431274)]\n",
      "lunch\n"
     ]
    }
   ],
   "source": [
    "print(google_model.most_similar('cricket', topn=5))\n",
    "#find the odd one out\n",
    "print (google_model.doesnt_match(\"cricket football sports lunch kohli\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSNE is pretty useful when it comes to visualizing similarity between objects. It works by taking a group of high-dimensional  vocabulary word feature vectors, then compresses them down to 2-dimensional x,y coordinate pairs. The idea is to keep similar words close together on the plane, while maximizing the distance between dissimilar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define function to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(sentences=None, size=100, window=5, min_count=5):\n",
    "#'''Creates and TSNE model and plots it\n",
    "#size: (default 100) The number of dimensions of the embedding, \n",
    "#e.g. the length of the dense vector to represent each token (word).\n",
    "#window: (default 5) The maximum distance between a target word and words around the target word.\n",
    "#min_count: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n",
    "#workers: (default 3) The number of threads to use while training.\n",
    "#sg: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1).\n",
    "#'''\n",
    "    model = Word2Vec(sentences,size = size,window=window, min_count=min_count)\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A more selective model\n",
    "model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=500, workers=4)\n",
    "#normal\n",
    "model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=200, workers=4) \n",
    "# A less selective model\n",
    "model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=100, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne_plot(sentences=brown.sents(), size=10, window=2, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
