{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import multiprocessing\n",
    "import re\n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>one has european accent either because doesn exist there are accents from europe but not european accent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mid twenties male rocking skinny jeans pants h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>honestly wouldn have believed didn live she ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>money just driver license credit cards and sub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>smoking tobacco went from shitty pall malls ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>that one reason for but not the only one the o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  one has european accent either because doesn exist there are accents from europe but not european accent\n",
       "0  mid twenties male rocking skinny jeans pants h...                                                      \n",
       "1  honestly wouldn have believed didn live she ma...                                                      \n",
       "2  money just driver license credit cards and sub...                                                      \n",
       "3  smoking tobacco went from shitty pall malls ma...                                                      \n",
       "4  that one reason for but not the only one the o...                                                      "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"C:/Users/ankit.bhatia/Google Drive/Python/Python3Scripts/problems/data/reddit-small.txt\",delimiter=\"/t\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(inp, out):\n",
    "    logger = logging.getLogger(\"word2vect-training\")\n",
    "    logging.basicConfig(format=\"%(asctime)s:%(levelname)s:%(message)s\")\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    \n",
    "    model = Word2Vec(LineSentence(inp), size=100, window=5,min_count=5,workers=multiprocessing.cpu_count())\n",
    "    model.init_sims(replace = True)\n",
    "    model.save(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-23 12:59:06,005:INFO:collecting all words and their counts\n",
      "2017-09-23 12:59:06,006:INFO:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-09-23 12:59:06,055:INFO:collected 11440 word types from a corpus of 105198 raw words and 5000 sentences\n",
      "2017-09-23 12:59:06,055:INFO:Loading a fresh vocabulary\n",
      "2017-09-23 12:59:06,065:INFO:min_count=5 retains 2362 unique words (20% of original 11440, drops 9078)\n",
      "2017-09-23 12:59:06,066:INFO:min_count=5 leaves 90968 word corpus (86% of original 105198, drops 14230)\n",
      "2017-09-23 12:59:06,073:INFO:deleting the raw counts dictionary of 11440 items\n",
      "2017-09-23 12:59:06,074:INFO:sample=0.001 downsamples 55 most-common words\n",
      "2017-09-23 12:59:06,075:INFO:downsampling leaves estimated 70796 word corpus (77.8% of prior 90968)\n",
      "2017-09-23 12:59:06,076:INFO:estimated required memory for 2362 words and 100 dimensions: 3070600 bytes\n",
      "2017-09-23 12:59:06,085:INFO:resetting layer weights\n",
      "2017-09-23 12:59:06,114:INFO:training model with 4 workers on 2362 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-09-23 12:59:06,413:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2017-09-23 12:59:06,413:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2017-09-23 12:59:06,431:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2017-09-23 12:59:06,434:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2017-09-23 12:59:06,434:INFO:training on 525990 raw words (354006 effective words) took 0.3s, 1114487 effective words/s\n",
      "2017-09-23 12:59:06,435:INFO:precomputing L2-norms of word weight vectors\n",
      "2017-09-23 12:59:06,448:INFO:saving Word2Vec object under C:/Users/ankit.bhatia/Google Drive/Python/Python3Scripts/problems/data/word-vec_out, separately None\n",
      "2017-09-23 12:59:06,449:INFO:not storing attribute syn0norm\n",
      "2017-09-23 12:59:06,450:INFO:not storing attribute cum_table\n",
      "2017-09-23 12:59:06,472:INFO:saved C:/Users/ankit.bhatia/Google Drive/Python/Python3Scripts/problems/data/word-vec_out\n"
     ]
    }
   ],
   "source": [
    "train_model(inp = \"C:/Users/ankit.bhatia/Google Drive/Python/Python3Scripts/problems/data/reddit-small.txt\",\n",
    "            out = \"C:/Users/ankit.bhatia/Google Drive/Python/Python3Scripts/problems/data/word-vec_out\"   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-23 12:59:23,675:INFO:loading Word2Vec object from C:/Users/ankit.bhatia/Google Drive/Python/Python3Scripts/problems/data/word-vec_out\n",
      "2017-09-23 12:59:23,694:INFO:loading wv recursively from C:/Users/ankit.bhatia/Google Drive/Python/Python3Scripts/problems/data/word-vec_out.wv.* with mmap=None\n",
      "2017-09-23 12:59:23,695:INFO:setting ignored attribute syn0norm to None\n",
      "2017-09-23 12:59:23,695:INFO:setting ignored attribute cum_table to None\n",
      "2017-09-23 12:59:23,696:INFO:loaded C:/Users/ankit.bhatia/Google Drive/Python/Python3Scripts/problems/data/word-vec_out\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(\"C:/Users/ankit.bhatia/Google Drive/Python/Python3Scripts/problems/data/word-vec_out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06186875, -0.10799092, -0.0434675 ,  0.01551846, -0.07273468,\n",
       "       -0.0481083 ,  0.14487197, -0.00403988, -0.04084463,  0.05320524,\n",
       "        0.04461222, -0.07346458, -0.0405463 , -0.09845059,  0.23961011,\n",
       "       -0.02402367, -0.10685134, -0.08968364,  0.07157437, -0.21893127,\n",
       "       -0.03603324,  0.04441294, -0.08726186, -0.02862711,  0.24644352,\n",
       "       -0.09143753,  0.14241964, -0.09720461, -0.00752129,  0.11016157,\n",
       "        0.08742291, -0.02727025, -0.03346224, -0.05516127,  0.05590902,\n",
       "        0.06821543,  0.05229606,  0.13278207,  0.23174331, -0.0193922 ,\n",
       "        0.05302497, -0.13244806,  0.05422791,  0.07121805, -0.06889854,\n",
       "       -0.13196521, -0.0111764 ,  0.01310601, -0.02966356,  0.06073076,\n",
       "        0.21385582,  0.12039542,  0.09564544, -0.13684128, -0.04855221,\n",
       "       -0.11004297,  0.06176914,  0.14413998, -0.01510142,  0.06133194,\n",
       "       -0.12508468,  0.07911213, -0.16886418, -0.01911671,  0.01768418,\n",
       "       -0.03700586, -0.02625733,  0.07822195,  0.11991667, -0.03389078,\n",
       "        0.01346369,  0.03897636,  0.05166635, -0.02663226, -0.14480126,\n",
       "        0.07968494, -0.29408509, -0.04281318,  0.09169351, -0.15951183,\n",
       "        0.05834679,  0.13764057,  0.13313992, -0.02025251, -0.19447245,\n",
       "       -0.07819688, -0.13474795, -0.01829164,  0.00352326, -0.03085747,\n",
       "        0.09339456,  0.04532942,  0.15724236, -0.05387175, -0.10403962,\n",
       "        0.05340137, -0.0317903 , -0.02916623,  0.08789726,  0.03293167], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['money']\n",
    "#model[['money','credit']]\n",
    "#type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('makes', 0.9999043345451355),\n",
       " ('different', 0.9998958110809326),\n",
       " ('isn', 0.9998947381973267),\n",
       " ('either', 0.9998929500579834),\n",
       " ('yeah', 0.9998908042907715),\n",
       " ('down', 0.9998906850814819),\n",
       " ('which', 0.9998897910118103),\n",
       " ('use', 0.9998893141746521),\n",
       " ('love', 0.999889075756073),\n",
       " ('actually', 0.9998873472213745)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('money')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99963254920818367"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('money','credit')\n",
    "#type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(inp1, inp2):\n",
    "    return np.dot(inp1, inp2) / (np.linalg.norm(inp1)*np.linalg.norm(inp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87885343166569463"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([1,5],[5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def average_similarity(text1, text2):\n",
    "    # Lower and tokenize the words\n",
    "    text1 = text1.lower().split()\n",
    "    text2 = text2.lower().split()\n",
    "    \n",
    "    # Get a list of word vectors for each word in the sentence\n",
    "    vector1 = np.array([model[word] for word in text1])\n",
    "    vector2 = np.array([model[word] for word in text2])\n",
    "    avg1_vector1 = np.mean(vector1,axis =0)\n",
    "    avg1_vector2 = np.mean(vector2,axis =0)\n",
    "    return cosine_similarity(avg1_vector1,avg1_vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99988896"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_similarity('money','love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
