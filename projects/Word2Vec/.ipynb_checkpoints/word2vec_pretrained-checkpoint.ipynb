{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building word2vec model from scratch requires lots and lots of data to train the model. Hence the model works best on google news corpus or Wikipedia corpus. As these are very large corpus and so it can not be trained on PCâ€™s with 8GB-16GB RAM. So I am using <b>nltk brown corpus</b> for the training. It has about <b>1.2 million words</b> and the results are good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy Version 1.12.1\n",
      "Pandas Version 0.20.3\n",
      "Tensorflow Version 1.1.0\n",
      "Keras Version 2.1.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Numpy Version '+np.__version__)\n",
    "import pandas as pd\n",
    "print('Pandas Version '+pd.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "print('Tensorflow Version '+tf.__version__)\n",
    "from IPython.display import Image # To view image from location/url\n",
    "import keras\n",
    "print('Keras Version '+keras.__version__)\n",
    "import nltk\n",
    "import logging\n",
    "import multiprocessing\n",
    "import re\n",
    "import os\n",
    "import gensim\n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from nltk.corpus import brown, movie_reviews, treebank\n",
    "#nltk.download('brown')\n",
    "#nltk.download('movie_reviews')\n",
    "#nltk.download('treebank')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Input & Output files path\n",
    "inputDir = \"./data/\"\n",
    "outputDir = \"./data/outDir/\"\n",
    "inputDir_BigFiles = \"E:/BigFiles/\"\n",
    "outputDir_Bigfiles = \"E:/BigFiles/outDir/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Load & Save NLTK model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown corpus model saved at ./data/outDir/\n",
      "Size of file :19197.9 KB\n"
     ]
    }
   ],
   "source": [
    "brown_model = Word2Vec(brown.sents())\n",
    "movie_reviews_model = Word2Vec(movie_reviews.sents())\n",
    "treebank_model = Word2Vec(treebank.sents())\n",
    "brown_model.save(outputDir+\"brown_model\")\n",
    "size = str(round((os.path.getsize(outputDir+'brown_model')/1000),1))\n",
    "print('Brown corpus model saved at '+outputDir+ '\\nSize of file :'+size+ ' KB' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test NLTK model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('care', 0.9140259027481079), ('chance', 0.9021503925323486), ('job', 0.8922946453094482), ('trouble', 0.8632405996322632), ('easy', 0.860753059387207)]\n",
      "cereal\n"
     ]
    }
   ],
   "source": [
    "# Brown\n",
    "print(brown_model.most_similar('money', topn=5))\n",
    "#find the odd one out\n",
    "print (brown_model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n",
    "#vector representation of word human\n",
    "#print (brown_model[\"human\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('attention', 0.7723175287246704), ('eyes', 0.7617353200912476), ('chance', 0.7536006569862366), ('him', 0.7455449104309082), ('home', 0.7288988828659058)]\n",
      "movie\n"
     ]
    }
   ],
   "source": [
    "# Movie Reviews\n",
    "print(movie_reviews_model.most_similar('money', topn=5))\n",
    "#find the odd one out\n",
    "print (movie_reviews_model.doesnt_match(\"girl boy her movie\".split()))\n",
    "#vector representation of word human\n",
    "#print (movie_reviews_model[\"human\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('all', 0.9999011158943176), ('only', 0.9998963475227356), ('new', 0.9998949766159058), ('some', 0.999890923500061), (\"'\", 0.9998877048492432)]\n",
      "money\n"
     ]
    }
   ],
   "source": [
    "# Treebank\n",
    "print(treebank_model.most_similar('money', topn=5))\n",
    "#find the odd one out\n",
    "print (treebank_model.doesnt_match(\"financial money earning fat\".split()))\n",
    "#vector representation of word human\n",
    "#print (treebank_model[\"human\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(inp, out, type=0):\n",
    "    '''\n",
    "    inp  : Input Dataset\n",
    "    out  : Output Model\n",
    "    type : 0(default) for CBOW & 1 for Skipgram\n",
    "    '''\n",
    "    logger = logging.getLogger(\"word2vect-training\")\n",
    "    logging.basicConfig(format=\"%(asctime)s:%(levelname)s:%(message)s\")\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    \n",
    "    model = Word2Vec(LineSentence(inp), size=100, window=5,min_count=5,workers=multiprocessing.cpu_count(),sg=type)\n",
    "    model.init_sims(replace = True)\n",
    "    model.save(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Load & Save Glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert glove_input_file in GloVe format into word2vec_output_file in word2vec format.\n",
    "gensim.scripts.glove2word2vec.glove2word2vec(glove_input_file = inputDir_BigFiles+\"glove.6B/glove.6B.50d.txt\", \n",
    "                                             word2vec_output_file = outputDir_Bigfiles+\"word2vec_glove_file\")\n",
    "\n",
    "# Load GloVe: Global Vectors for Word Representation Word2Vec model.\n",
    "glove_model = word2vec.KeyedVectors.load_word2vec_format(outputDir_Bigfiles+\"word2vec_glove_file\")\n",
    "glove_model.save(outputDir_Bigfiles+\"word2vec_glove_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cash', 0.8989869356155396), ('paying', 0.8788655996322632), ('funds', 0.8768925070762634), ('pay', 0.8716533184051514), ('raise', 0.8444491028785706)]\n",
      "cereal\n"
     ]
    }
   ],
   "source": [
    "print(glove_model.most_similar('money', topn=5))\n",
    "#find the odd one out\n",
    "print (glove_model.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "google_model = word2vec.KeyedVectors.load_word2vec_format('D:/Drive/BigFiles/GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_model.save(\"D:/Drive/BigFiles/Google/google_word-vec_out.syn0.npy\")\n",
    "print (\"Google corpus model saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ntpath' from 'C:\\\\Users\\\\ankit.bhatia\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\tensor\\\\lib\\\\ntpath.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IRIS-CSG-1387'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import socket\n",
    "host_name = socket.gethostname()\n",
    "host_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
