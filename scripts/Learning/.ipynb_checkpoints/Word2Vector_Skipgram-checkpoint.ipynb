{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
=======
      "C:\\Users\\ab9bh\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
>>>>>>> 2e41b9d0e4d5c4e1997662706d3b6459802dc901
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import multiprocessing\n",
    "import re\n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
<<<<<<< HEAD
    "dirpath = 'C:/Users/ankit.bhatia/Documents/GitHub/PythonScripts/data/'"
=======
    "dirpath = './data/'"
>>>>>>> 2e41b9d0e4d5c4e1997662706d3b6459802dc901
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 3,
>>>>>>> 2e41b9d0e4d5c4e1997662706d3b6459802dc901
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
=======
      "C:\\Users\\ab9bh\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
>>>>>>> 2e41b9d0e4d5c4e1997662706d3b6459802dc901
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>one has european accent either because doesn exist there are accents from europe but not european accent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mid twenties male rocking skinny jeans pants h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>honestly wouldn have believed didn live she ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>money just driver license credit cards and sub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>smoking tobacco went from shitty pall malls ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>that one reason for but not the only one the o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  one has european accent either because doesn exist there are accents from europe but not european accent\n",
       "0  mid twenties male rocking skinny jeans pants h...                                                      \n",
       "1  honestly wouldn have believed didn live she ma...                                                      \n",
       "2  money just driver license credit cards and sub...                                                      \n",
       "3  smoking tobacco went from shitty pall malls ma...                                                      \n",
       "4  that one reason for but not the only one the o...                                                      "
      ]
     },
<<<<<<< HEAD
     "execution_count": 2,
=======
     "execution_count": 3,
>>>>>>> 2e41b9d0e4d5c4e1997662706d3b6459802dc901
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(dirpath+'reddit-small.txt',delimiter=\"/t\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 15,
>>>>>>> 2e41b9d0e4d5c4e1997662706d3b6459802dc901
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(inp, out):\n",
    "    logger = logging.getLogger(\"word2vect-training\")\n",
    "    logging.basicConfig(format=\"%(asctime)s:%(levelname)s:%(message)s\")\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    \n",
<<<<<<< HEAD
    "    model = Word2Vec(LineSentence(inp), size=100, window=5,min_count=5,workers=multiprocessing.cpu_count())\n",
=======
    "    model = Word2Vec(LineSentence(inp), size=100, window=5,min_count=5,workers=multiprocessing.cpu_count(),sg=1)\n",
>>>>>>> 2e41b9d0e4d5c4e1997662706d3b6459802dc901
    "    model.init_sims(replace = True)\n",
    "    model.save(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "2017-09-24 11:23:30,589:INFO:collecting all words and their counts\n",
      "2017-09-24 11:23:30,591:INFO:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-09-24 11:23:30,691:INFO:collected 11440 word types from a corpus of 105198 raw words and 5000 sentences\n",
      "2017-09-24 11:23:30,691:INFO:Loading a fresh vocabulary\n",
      "2017-09-24 11:23:30,712:INFO:min_count=5 retains 2362 unique words (20% of original 11440, drops 9078)\n",
      "2017-09-24 11:23:30,718:INFO:min_count=5 leaves 90968 word corpus (86% of original 105198, drops 14230)\n",
      "2017-09-24 11:23:30,754:INFO:deleting the raw counts dictionary of 11440 items\n",
      "2017-09-24 11:23:30,756:INFO:sample=0.001 downsamples 55 most-common words\n",
      "2017-09-24 11:23:30,757:INFO:downsampling leaves estimated 70796 word corpus (77.8% of prior 90968)\n",
      "2017-09-24 11:23:30,763:INFO:estimated required memory for 2362 words and 100 dimensions: 3070600 bytes\n",
      "2017-09-24 11:23:30,786:INFO:resetting layer weights\n",
      "2017-09-24 11:23:30,930:INFO:training model with 4 workers on 2362 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-09-24 11:23:31,758:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2017-09-24 11:23:31,758:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2017-09-24 11:23:31,773:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2017-09-24 11:23:31,773:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2017-09-24 11:23:31,773:INFO:training on 525990 raw words (354129 effective words) took 0.8s, 419128 effective words/s\n",
      "2017-09-24 11:23:31,773:INFO:precomputing L2-norms of word weight vectors\n",
      "2017-09-24 11:23:31,820:INFO:saving Word2Vec object under C:/Users/ankit.bhatia/Documents/GitHub/PythonScripts/data/word-vec_out, separately None\n",
      "2017-09-24 11:23:31,820:INFO:not storing attribute syn0norm\n",
      "2017-09-24 11:23:31,820:INFO:not storing attribute cum_table\n",
      "2017-09-24 11:23:31,870:INFO:saved C:/Users/ankit.bhatia/Documents/GitHub/PythonScripts/data/word-vec_out\n"
=======
      "2018-01-28 17:54:48,428:INFO:collecting all words and their counts\n",
      "2018-01-28 17:54:48,430:INFO:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-01-28 17:54:48,475:INFO:collected 11440 word types from a corpus of 105198 raw words and 5000 sentences\n",
      "2018-01-28 17:54:48,475:INFO:Loading a fresh vocabulary\n",
      "2018-01-28 17:54:48,487:INFO:min_count=5 retains 2362 unique words (20% of original 11440, drops 9078)\n",
      "2018-01-28 17:54:48,489:INFO:min_count=5 leaves 90968 word corpus (86% of original 105198, drops 14230)\n",
      "2018-01-28 17:54:48,498:INFO:deleting the raw counts dictionary of 11440 items\n",
      "2018-01-28 17:54:48,500:INFO:sample=0.001 downsamples 55 most-common words\n",
      "2018-01-28 17:54:48,502:INFO:downsampling leaves estimated 70796 word corpus (77.8% of prior 90968)\n",
      "2018-01-28 17:54:48,504:INFO:estimated required memory for 2362 words and 100 dimensions: 3070600 bytes\n",
      "2018-01-28 17:54:48,513:INFO:resetting layer weights\n",
      "2018-01-28 17:54:48,541:INFO:training model with 8 workers on 2362 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-01-28 17:54:48,995:INFO:worker thread finished; awaiting finish of 7 more threads\n",
      "2018-01-28 17:54:48,999:INFO:worker thread finished; awaiting finish of 6 more threads\n",
      "2018-01-28 17:54:49,007:INFO:worker thread finished; awaiting finish of 5 more threads\n",
      "2018-01-28 17:54:49,023:INFO:worker thread finished; awaiting finish of 4 more threads\n",
      "2018-01-28 17:54:49,025:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2018-01-28 17:54:49,027:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-28 17:54:49,031:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-28 17:54:49,033:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-28 17:54:49,035:INFO:training on 525990 raw words (354077 effective words) took 0.5s, 725861 effective words/s\n",
      "2018-01-28 17:54:49,037:WARNING:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-28 17:54:49,040:INFO:precomputing L2-norms of word weight vectors\n",
      "2018-01-28 17:54:49,054:INFO:saving Word2Vec object under ./data/word-vec_out, separately None\n",
      "2018-01-28 17:54:49,056:INFO:not storing attribute syn0norm\n",
      "2018-01-28 17:54:49,057:INFO:not storing attribute cum_table\n",
      "2018-01-28 17:54:49,085:INFO:saved ./data/word-vec_out\n"
>>>>>>> 2e41b9d0e4d5c4e1997662706d3b6459802dc901
     ]
    }
   ],
   "source": [
    "train_model(inp = dirpath+\"reddit-small.txt\",\n",
    "            out = dirpath+\"word-vec_out\"   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "2017-09-24 11:23:31,886:INFO:loading Word2Vec object from C:/Users/ankit.bhatia/Documents/GitHub/PythonScripts/data/word-vec_out\n",
      "2017-09-24 11:23:31,917:INFO:loading wv recursively from C:/Users/ankit.bhatia/Documents/GitHub/PythonScripts/data/word-vec_out.wv.* with mmap=None\n",
      "2017-09-24 11:23:31,918:INFO:setting ignored attribute syn0norm to None\n",
      "2017-09-24 11:23:31,919:INFO:setting ignored attribute cum_table to None\n",
      "2017-09-24 11:23:31,921:INFO:loaded C:/Users/ankit.bhatia/Documents/GitHub/PythonScripts/data/word-vec_out\n"
=======
      "2018-01-28 17:54:56,450:INFO:loading Word2Vec object from ./data/word-vec_out\n",
      "2018-01-28 17:54:56,473:INFO:loading wv recursively from ./data/word-vec_out.wv.* with mmap=None\n",
      "2018-01-28 17:54:56,474:INFO:setting ignored attribute syn0norm to None\n",
      "2018-01-28 17:54:56,475:INFO:setting ignored attribute cum_table to None\n",
      "2018-01-28 17:54:56,476:INFO:loaded ./data/word-vec_out\n"
>>>>>>> 2e41b9d0e4d5c4e1997662706d3b6459802dc901
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(dirpath+\"word-vec_out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "array([ 0.02319002,  0.0418434 ,  0.14794236,  0.0697709 , -0.24156402,\n",
       "        0.04989811,  0.04689566, -0.21430039, -0.02242533, -0.16577491,\n",
       "        0.05358276,  0.04728424,  0.10224799,  0.08473255, -0.09588826,\n",
       "       -0.11316206,  0.00327825, -0.06676696, -0.14914736,  0.10596239,\n",
       "        0.08772298,  0.09212939, -0.0622538 ,  0.10680088,  0.03008068,\n",
       "        0.00379439,  0.09999976, -0.04442582,  0.02415598,  0.04484349,\n",
       "       -0.03746192,  0.02424393, -0.02398343, -0.04545423, -0.14784239,\n",
       "       -0.03059241, -0.08003729,  0.10501141, -0.1119035 ,  0.25993574,\n",
       "       -0.0685288 , -0.07035118, -0.04251468, -0.12503135, -0.01649881,\n",
       "       -0.14230427, -0.03352469, -0.127736  ,  0.02077559,  0.03759067,\n",
       "        0.25818563, -0.16707052, -0.13014413, -0.13967511, -0.18230535,\n",
       "       -0.15794574, -0.04542919, -0.1387016 , -0.10376861,  0.06693321,\n",
       "       -0.20339879, -0.05599453,  0.0588149 , -0.11545787,  0.01000598,\n",
       "       -0.0019387 , -0.17436655,  0.01222221, -0.16408396,  0.02612096,\n",
       "       -0.05057011,  0.10448375,  0.06958102, -0.01778886, -0.04986407,\n",
       "       -0.04802602, -0.06065883,  0.11665498,  0.06089765, -0.15439226,\n",
       "       -0.17609535,  0.09979548,  0.04587156, -0.0775269 , -0.12112845,\n",
       "       -0.01042359, -0.04037168, -0.06886042, -0.0408367 , -0.05364933,\n",
       "        0.02189893,  0.04591889, -0.02729171, -0.03776848, -0.11023666,\n",
       "        0.03802116,  0.00998922,  0.01328028, -0.01030839, -0.07374758], dtype=float32)"
=======
       "array([ -7.49743432e-02,  -2.00432933e-05,   1.73459828e-01,\n",
       "        -2.40707528e-02,   1.14087099e-02,   1.66874558e-01,\n",
       "         1.40477298e-02,   9.11777318e-02,  -1.50288818e-02,\n",
       "        -1.40245169e-01,  -8.73620249e-03,  -8.77445787e-02,\n",
       "        -4.41877618e-02,  -3.96691598e-02,   7.90792052e-03,\n",
       "        -2.85680909e-02,   1.39711872e-02,  -8.88911858e-02,\n",
       "        -2.48933937e-02,   9.08740088e-02,  -6.35724068e-02,\n",
       "        -1.78281236e-02,  -3.01272217e-02,  -1.48916483e-01,\n",
       "         1.04077548e-01,   9.48925316e-02,  -8.88362452e-02,\n",
       "         2.00886950e-02,   1.12948164e-01,  -2.98786629e-02,\n",
       "        -6.80129975e-02,  -2.29056895e-01,   8.94078463e-02,\n",
       "        -1.82199273e-02,   3.09630394e-01,   1.89001337e-01,\n",
       "         2.19391391e-01,  -7.23471120e-02,   2.40034554e-02,\n",
       "         3.79985385e-02,  -7.65863881e-02,   2.99364142e-03,\n",
       "        -1.64342299e-01,   7.81288072e-02,  -8.69865939e-02,\n",
       "         6.39869645e-02,  -7.16885505e-03,   2.09820364e-02,\n",
       "        -1.53699428e-01,   1.32444039e-01,   5.85457943e-02,\n",
       "        -9.28356797e-02,   4.75613996e-02,  -1.50396705e-01,\n",
       "        -8.34094584e-02,   1.11194797e-01,  -3.70786227e-02,\n",
       "         1.20996889e-02,   1.22070447e-01,   9.41217840e-02,\n",
       "         9.18570012e-02,   1.13303885e-01,  -2.41513461e-01,\n",
       "         3.55962315e-03,   6.15612455e-02,   5.77650890e-02,\n",
       "         1.75921783e-01,  -7.13743120e-02,   1.47048850e-02,\n",
       "        -1.38345972e-01,   2.22399384e-02,   1.44295484e-01,\n",
       "        -5.80061823e-02,  -1.89820230e-02,  -1.05129577e-01,\n",
       "        -1.37453094e-01,  -1.18076868e-01,  -8.31854045e-02,\n",
       "         4.26414944e-02,  -9.13392007e-03,   3.10703530e-04,\n",
       "         1.22234501e-01,  -7.25000203e-02,   8.06057267e-03,\n",
       "         1.61740199e-01,  -8.32053274e-02,   6.75589964e-02,\n",
       "        -9.80093554e-02,  -3.12533528e-02,   5.32782376e-02,\n",
       "         1.85311101e-02,  -5.20586409e-02,  -1.46591095e-02,\n",
       "         9.25102830e-02,  -6.00297563e-02,   1.77519947e-01,\n",
       "        -3.66536304e-02,   1.17905416e-01,  -1.66650251e-01,\n",
       "        -3.91410291e-02], dtype=float32)"
>>>>>>> 2e41b9d0e4d5c4e1997662706d3b6459802dc901
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['money']\n",
    "#model[['money','credit']]\n",
    "#type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "2017-09-24 11:23:32,235:INFO:precomputing L2-norms of word weight vectors\n"
=======
      "2018-01-28 17:55:03,662:INFO:precomputing L2-norms of word weight vectors\n"
>>>>>>> 2e41b9d0e4d5c4e1997662706d3b6459802dc901
     ]
    },
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "[('most', 0.999903678894043),\n",
       " ('body', 0.9999003410339355),\n",
       " ('true', 0.9998928904533386),\n",
       " ('stuff', 0.9998921155929565),\n",
       " ('different', 0.9998879432678223),\n",
       " ('either', 0.9998877644538879),\n",
       " ('myself', 0.9998834729194641),\n",
       " ('which', 0.9998819828033447),\n",
       " ('having', 0.9998794198036194),\n",
       " ('use', 0.9998781681060791)]"
=======
       "[('important', 0.993507981300354),\n",
       " ('without', 0.9922817945480347),\n",
       " ('likely', 0.9921600222587585),\n",
       " ('change', 0.9914062023162842),\n",
       " ('tips', 0.991248369216919),\n",
       " ('money', 0.9912190437316895),\n",
       " ('aren', 0.9910928010940552),\n",
       " ('americans', 0.9907670021057129),\n",
       " ('lazy', 0.9902044534683228),\n",
       " ('others', 0.9897154569625854)]"
>>>>>>> 2e41b9d0e4d5c4e1997662706d3b6459802dc901
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "model.most_similar('money')"
=======
    "model.most_similar('women')"
>>>>>>> 2e41b9d0e4d5c4e1997662706d3b6459802dc901
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "0.99961849534116176"
=======
       "0.93953390158382089"
>>>>>>> 2e41b9d0e4d5c4e1997662706d3b6459802dc901
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('money','credit')\n",
    "#type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(inp1, inp2):\n",
    "    return np.dot(inp1, inp2) / (np.linalg.norm(inp1)*np.linalg.norm(inp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87885343166569463"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([1,5],[5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_similarity(text1, text2):\n",
    "    # Lower and tokenize the words\n",
    "    text1 = text1.lower().split()\n",
    "    text2 = text2.lower().split()\n",
    "    \n",
    "    # Get a list of word vectors for each word in the sentence\n",
    "    vector1 = np.array([model[word] for word in text1])\n",
    "    vector2 = np.array([model[word] for word in text2])\n",
    "    avg1_vector1 = np.mean(vector1,axis =0)\n",
    "    avg1_vector2 = np.mean(vector2,axis =0)\n",
    "    return cosine_similarity(avg1_vector1,avg1_vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "0.99984962"
=======
       "0.95792466"
>>>>>>> 2e41b9d0e4d5c4e1997662706d3b6459802dc901
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_similarity('money','love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
