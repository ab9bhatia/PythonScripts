{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building word2vec model from scratch requires lots and lots of data to train the model. Hence the model works best on google news corpus or Wikipedia corpus. As these are very large corpus and so it can not be trained on PCâ€™s with 8GB-16GB RAM. So I am using <b>nltk brown corpus</b> for the training. It has about <b>1.2 million words</b> and the results are good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ankit.bhatia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from nltk.corpus import brown, movie_reviews, treebank\n",
    "#nltk.download('brown')\n",
    "#nltk.download('movie_reviews')\n",
    "#nltk.download('treebank')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_model = Word2Vec(brown.sents())\n",
    "movie_reviews_model = Word2Vec(movie_reviews.sents())\n",
    "treebank_model = Word2Vec(treebank.sents())\n",
    "#model.save(\"./data/nltk_brown_model\")\n",
    "#print (\"Brown corpus model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('care', 0.9140259027481079), ('chance', 0.9021503925323486), ('job', 0.8922946453094482), ('trouble', 0.8632405996322632), ('easy', 0.860753059387207)]\n",
      "cereal\n"
     ]
    }
   ],
   "source": [
    "# Brown\n",
    "print(brown_model.most_similar('money', topn=5))\n",
    "#find the odd one out\n",
    "print (brown_model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n",
    "#vector representation of word human\n",
    "#print (brown_model[\"human\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('attention', 0.7723175287246704), ('eyes', 0.7617353200912476), ('chance', 0.7536006569862366), ('him', 0.7455449104309082), ('home', 0.7288988828659058)]\n",
      "movie\n"
     ]
    }
   ],
   "source": [
    "# Movie Reviews\n",
    "print(movie_reviews_model.most_similar('money', topn=5))\n",
    "#find the odd one out\n",
    "print (movie_reviews_model.doesnt_match(\"girl boy her movie\".split()))\n",
    "#vector representation of word human\n",
    "#print (movie_reviews_model[\"human\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('all', 0.9999011158943176), ('only', 0.9998963475227356), ('new', 0.9998949766159058), ('some', 0.999890923500061), (\"'\", 0.9998877048492432)]\n",
      "money\n"
     ]
    }
   ],
   "source": [
    "# Treebank\n",
    "print(treebank_model.most_similar('money', topn=5))\n",
    "#find the odd one out\n",
    "print (treebank_model.doesnt_match(\"financial money earning fat\".split()))\n",
    "#vector representation of word human\n",
    "#print (treebank_model[\"human\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(inp, out, type=0):\n",
    "    '''\n",
    "    inp  : Input Dataset\n",
    "    out  : Output Model\n",
    "    type : 0(default) for CBOW & 1 for Skipgram\n",
    "    '''\n",
    "    logger = logging.getLogger(\"word2vect-training\")\n",
    "    logging.basicConfig(format=\"%(asctime)s:%(levelname)s:%(message)s\")\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    \n",
    "    model = Word2Vec(LineSentence(inp), size=100, window=5,min_count=5,workers=multiprocessing.cpu_count(),sg=type)\n",
    "    model.init_sims(replace = True)\n",
    "    model.save(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "google_model = word2vec.KeyedVectors.load_word2vec_format('D:/Drive/BigFiles/GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_model.save(\"D:/Drive/BigFiles/Google/google_word-vec_out.syn0.npy\")\n",
    "print (\"Google corpus model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
