{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import multiprocessing\n",
    "import re\n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "dirpath = 'C:/Users/ankit.bhatia/Documents/GitHub/PythonScripts/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>one has european accent either because doesn exist there are accents from europe but not european accent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mid twenties male rocking skinny jeans pants h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>honestly wouldn have believed didn live she ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>money just driver license credit cards and sub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>smoking tobacco went from shitty pall malls ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>that one reason for but not the only one the o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  one has european accent either because doesn exist there are accents from europe but not european accent\n",
       "0  mid twenties male rocking skinny jeans pants h...                                                      \n",
       "1  honestly wouldn have believed didn live she ma...                                                      \n",
       "2  money just driver license credit cards and sub...                                                      \n",
       "3  smoking tobacco went from shitty pall malls ma...                                                      \n",
       "4  that one reason for but not the only one the o...                                                      "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(dirpath+'reddit-small.txt',delimiter=\"/t\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(inp, out):\n",
    "    logger = logging.getLogger(\"word2vect-training\")\n",
    "    logging.basicConfig(format=\"%(asctime)s:%(levelname)s:%(message)s\")\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    \n",
    "    model = Word2Vec(LineSentence(inp), size=100, window=5,min_count=5,workers=multiprocessing.cpu_count())\n",
    "    model.init_sims(replace = True)\n",
    "    model.save(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0e73a4e02a6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_model(inp = dirpath+\"reddit-small.txt\",\n\u001b[0m\u001b[1;32m      2\u001b[0m             out = dirpath+\"word-vec_out\"   )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"
     ]
    }
   ],
   "source": [
    "train_model(inp = dirpath+\"reddit-small.txt\",\n",
    "            out = dirpath+\"word-vec_out\"   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-23 12:59:23,675:INFO:loading Word2Vec object from C:/Users/ankit.bhatia/Google Drive/Python/Python3Scripts/problems/data/word-vec_out\n",
      "2017-09-23 12:59:23,694:INFO:loading wv recursively from C:/Users/ankit.bhatia/Google Drive/Python/Python3Scripts/problems/data/word-vec_out.wv.* with mmap=None\n",
      "2017-09-23 12:59:23,695:INFO:setting ignored attribute syn0norm to None\n",
      "2017-09-23 12:59:23,695:INFO:setting ignored attribute cum_table to None\n",
      "2017-09-23 12:59:23,696:INFO:loaded C:/Users/ankit.bhatia/Google Drive/Python/Python3Scripts/problems/data/word-vec_out\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(dirpath+\"word-vec_out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06186875, -0.10799092, -0.0434675 ,  0.01551846, -0.07273468,\n",
       "       -0.0481083 ,  0.14487197, -0.00403988, -0.04084463,  0.05320524,\n",
       "        0.04461222, -0.07346458, -0.0405463 , -0.09845059,  0.23961011,\n",
       "       -0.02402367, -0.10685134, -0.08968364,  0.07157437, -0.21893127,\n",
       "       -0.03603324,  0.04441294, -0.08726186, -0.02862711,  0.24644352,\n",
       "       -0.09143753,  0.14241964, -0.09720461, -0.00752129,  0.11016157,\n",
       "        0.08742291, -0.02727025, -0.03346224, -0.05516127,  0.05590902,\n",
       "        0.06821543,  0.05229606,  0.13278207,  0.23174331, -0.0193922 ,\n",
       "        0.05302497, -0.13244806,  0.05422791,  0.07121805, -0.06889854,\n",
       "       -0.13196521, -0.0111764 ,  0.01310601, -0.02966356,  0.06073076,\n",
       "        0.21385582,  0.12039542,  0.09564544, -0.13684128, -0.04855221,\n",
       "       -0.11004297,  0.06176914,  0.14413998, -0.01510142,  0.06133194,\n",
       "       -0.12508468,  0.07911213, -0.16886418, -0.01911671,  0.01768418,\n",
       "       -0.03700586, -0.02625733,  0.07822195,  0.11991667, -0.03389078,\n",
       "        0.01346369,  0.03897636,  0.05166635, -0.02663226, -0.14480126,\n",
       "        0.07968494, -0.29408509, -0.04281318,  0.09169351, -0.15951183,\n",
       "        0.05834679,  0.13764057,  0.13313992, -0.02025251, -0.19447245,\n",
       "       -0.07819688, -0.13474795, -0.01829164,  0.00352326, -0.03085747,\n",
       "        0.09339456,  0.04532942,  0.15724236, -0.05387175, -0.10403962,\n",
       "        0.05340137, -0.0317903 , -0.02916623,  0.08789726,  0.03293167], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['money']\n",
    "#model[['money','credit']]\n",
    "#type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('makes', 0.9999043345451355),\n",
       " ('different', 0.9998958110809326),\n",
       " ('isn', 0.9998947381973267),\n",
       " ('either', 0.9998929500579834),\n",
       " ('yeah', 0.9998908042907715),\n",
       " ('down', 0.9998906850814819),\n",
       " ('which', 0.9998897910118103),\n",
       " ('use', 0.9998893141746521),\n",
       " ('love', 0.999889075756073),\n",
       " ('actually', 0.9998873472213745)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('money')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99963254920818367"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('money','credit')\n",
    "#type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(inp1, inp2):\n",
    "    return np.dot(inp1, inp2) / (np.linalg.norm(inp1)*np.linalg.norm(inp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87885343166569463"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([1,5],[5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def average_similarity(text1, text2):\n",
    "    # Lower and tokenize the words\n",
    "    text1 = text1.lower().split()\n",
    "    text2 = text2.lower().split()\n",
    "    \n",
    "    # Get a list of word vectors for each word in the sentence\n",
    "    vector1 = np.array([model[word] for word in text1])\n",
    "    vector2 = np.array([model[word] for word in text2])\n",
    "    avg1_vector1 = np.mean(vector1,axis =0)\n",
    "    avg1_vector2 = np.mean(vector2,axis =0)\n",
    "    return cosine_similarity(avg1_vector1,avg1_vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99988896"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_similarity('money','love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
