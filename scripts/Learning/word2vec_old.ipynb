{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized representation of words \n",
    "There are two ways to represent text data into numbers so that computer can understand that :\n",
    "1. *One hot encoder* : One hot encoders are binary, sparse(mostly made of zeros), and very high dimensional(same dimensionality as the number of words in the vocabulary)\n",
    "2. *Word Embeddings* : Word embeddings are low dimensional, floating point, dense vectors. So word embeddings pack more information into far fewer dimensions.\n",
    "\n",
    "![](./data/images/OneHotvsWordEmbedding.png \"OneHotvsWordEmbedding\")\n",
    "\n",
    "*One of the benefits of using dense and low-dimensional vectors is computational: the majority of neural network toolkits do not play well with very high-dimensional, sparse vectors. … The main benefit of the dense representations is generalization power: if we believe some features may provide similar clues, it is worthwhile to provide a representation that is able to capture these similarities.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec\n",
    "<b>Word2vec</b> is a group of related models that are used to produce <b>word embeddings</b>. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.\n",
    "\n",
    "![](./data/images/word2vec.png \"Word2Vec\")\n",
    "\n",
    "There are two ways to obtain word embeddings :\n",
    "\n",
    "1. <b>Train your own model from scratch</b>,In this setup you start with random word vectors and learn the word vectors in the same way you learn the weights of the neural netork.\n",
    "2. Load into your model word embeddings that were precomputed using a different machine-lerning task.These are called <b>pretrained word embeddings</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Word2Vec\n",
    "Word2Vec is one of the most widely used form of word vector representation.\n",
    "\n",
    "It has two variants:\n",
    "\n",
    "1. CBOW (Continuous Bag of Words) : This model tries to predict a word on bases of it’s neighbours.\n",
    "2. SkipGram : This models tries to predict the neighbours of a word.\n",
    "\n",
    "![](./data/images/CBOWvsSkipGram.png \"CBOWvsSkipGram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Gensim provides the Word2Vec class for working with a Word2Vec model.\n",
    "\n",
    "It only requires that the input must provide sentences sequentially, when iterated over. No need to keep everything in RAM: we can provide one sentence, process it, forget it, load another sentence.\n",
    "\n",
    "Learning a word embedding from text involves loading and organizing the text into sentences and providing them to the constructor of a new Word2Vec() instance.\n",
    "\n",
    "\n",
    "There are many parameters on this constructor; a few noteworthy arguments you may wish to configure are:\n",
    "\n",
    "* size: (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word).\n",
    "* window: (default 5) The maximum distance between a target word and words around the target word.\n",
    "* min_count: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n",
    "* workers: (default 3) The number of threads to use while training.\n",
    "* sg: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1).\n",
    "\n",
    "![](./data/images/word2vec_function.png \"word2vec_function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy Version 1.12.1\n",
      "Pandas Version 0.20.3\n",
      "Tensorflow Version 1.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Version 2.1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankit.bhatia\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensor\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Numpy Version '+np.__version__)\n",
    "import pandas as pd\n",
    "print('Pandas Version '+pd.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "print('Tensorflow Version '+tf.__version__)\n",
    "from IPython.display import Image # To view image from location/url\n",
    "import keras\n",
    "print('Keras Version '+keras.__version__)\n",
    "import nltk\n",
    "import logging\n",
    "import multiprocessing\n",
    "import re\n",
    "import os\n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dateset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankit.bhatia\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensor\\lib\\site-packages\\ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one has european accent either because doesn e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mid twenties male rocking skinny jeans pants h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>honestly wouldn have believed didn live she ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>money just driver license credit cards and sub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>smoking tobacco went from shitty pall malls ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  one has european accent either because doesn e...\n",
       "1  mid twenties male rocking skinny jeans pants h...\n",
       "2  honestly wouldn have believed didn live she ma...\n",
       "3  money just driver license credit cards and sub...\n",
       "4  smoking tobacco went from shitty pall malls ma..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('./data/reddit-small.txt',delimiter=\"/t\",header=None)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(inp, out, type=0):\n",
    "    '''\n",
    "    inp  : Input Dataset\n",
    "    out  : Output Model\n",
    "    type : 0(default) for CBOW & 1 for Skipgram\n",
    "    '''\n",
    "    logger = logging.getLogger(\"word2vect-training\")\n",
    "    logging.basicConfig(format=\"%(asctime)s:%(levelname)s:%(message)s\")\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    \n",
    "    model = Word2Vec(LineSentence(inp), size=100, window=5,min_count=5,workers=multiprocessing.cpu_count(),sg=type)\n",
    "    model.init_sims(replace = True)\n",
    "    model.save(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-29 12:02:01,620:INFO:collecting all words and their counts\n",
      "2018-01-29 12:02:01,635:INFO:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-01-29 12:02:01,763:INFO:collected 11440 word types from a corpus of 105198 raw words and 5000 sentences\n",
      "2018-01-29 12:02:01,763:INFO:Loading a fresh vocabulary\n",
      "2018-01-29 12:02:01,776:INFO:min_count=5 retains 2362 unique words (20% of original 11440, drops 9078)\n",
      "2018-01-29 12:02:01,776:INFO:min_count=5 leaves 90968 word corpus (86% of original 105198, drops 14230)\n",
      "2018-01-29 12:02:01,780:INFO:deleting the raw counts dictionary of 11440 items\n",
      "2018-01-29 12:02:01,784:INFO:sample=0.001 downsamples 55 most-common words\n",
      "2018-01-29 12:02:01,784:INFO:downsampling leaves estimated 70796 word corpus (77.8% of prior 90968)\n",
      "2018-01-29 12:02:01,788:INFO:estimated required memory for 2362 words and 100 dimensions: 3070600 bytes\n",
      "2018-01-29 12:02:01,795:INFO:resetting layer weights\n",
      "2018-01-29 12:02:01,818:INFO:training model with 4 workers on 2362 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-01-29 12:02:02,146:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2018-01-29 12:02:02,151:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-29 12:02:02,151:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-29 12:02:02,155:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-29 12:02:02,155:INFO:training on 525990 raw words (353246 effective words) took 0.3s, 1096819 effective words/s\n",
      "2018-01-29 12:02:02,155:INFO:precomputing L2-norms of word weight vectors\n",
      "2018-01-29 12:02:02,167:INFO:saving Word2Vec object under ./data/word-vec_out, separately None\n",
      "2018-01-29 12:02:02,167:INFO:not storing attribute syn0norm\n",
      "2018-01-29 12:02:02,171:INFO:not storing attribute cum_table\n",
      "2018-01-29 12:02:02,204:INFO:saved ./data/word-vec_out\n"
     ]
    }
   ],
   "source": [
    "train_model(inp = \"./data/reddit-small.txt\",\n",
    "            out = \"./data/word-vec_out\"   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-29 12:02:30,724:INFO:loading Word2Vec object from ./data/word-vec_out\n",
      "2018-01-29 12:02:30,783:INFO:loading wv recursively from ./data/word-vec_out.wv.* with mmap=None\n",
      "2018-01-29 12:02:30,783:INFO:setting ignored attribute syn0norm to None\n",
      "2018-01-29 12:02:30,783:INFO:setting ignored attribute cum_table to None\n",
      "2018-01-29 12:02:30,795:INFO:loaded ./data/word-vec_out\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(\"./data/word-vec_out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.90059885e-02,   1.04609676e-01,  -1.34053364e-01,\n",
       "         6.06180280e-02,   5.70349433e-02,   1.16132729e-01,\n",
       "         9.15558189e-02,   7.53469020e-02,   2.01130658e-02,\n",
       "        -3.30987535e-02,   2.42092852e-02,  -5.52618690e-02,\n",
       "         2.71960914e-01,   1.94251835e-01,  -2.35603247e-02,\n",
       "        -1.29673332e-01,  -1.92022620e-04,  -7.77240917e-02,\n",
       "         1.38281837e-01,  -1.00346036e-01,  -1.59813613e-02,\n",
       "        -1.28096864e-01,   1.03802122e-01,   1.58218250e-01,\n",
       "         2.25728042e-02,   9.43239406e-02,   1.56967223e-01,\n",
       "        -4.76592258e-02,   1.68878026e-02,  -1.11031830e-01,\n",
       "        -6.51366962e-03,  -9.57021117e-02,  -1.02031894e-01,\n",
       "        -6.38997927e-02,   1.97297111e-01,  -1.59684107e-01,\n",
       "        -4.18512151e-02,   1.58400852e-02,  -4.71418574e-02,\n",
       "         7.71993250e-02,   1.08654588e-01,  -7.62384664e-03,\n",
       "        -6.46656156e-02,   2.86474358e-03,   1.21822692e-01,\n",
       "         2.02864949e-02,   4.34008725e-02,   1.13713015e-02,\n",
       "         2.44429801e-02,   1.94011301e-01,  -7.09963739e-02,\n",
       "        -2.17731625e-01,  -6.87293615e-03,   8.65406096e-02,\n",
       "         2.66467463e-02,  -1.32006288e-01,  -9.82899517e-02,\n",
       "         8.44588969e-03,  -5.87915210e-03,  -1.10603943e-01,\n",
       "        -1.04143452e-02,  -1.34245120e-02,  -7.11254850e-02,\n",
       "         1.17721409e-01,  -6.83418214e-02,  -1.17172457e-01,\n",
       "         6.00442244e-03,   3.83637729e-03,  -3.98292169e-02,\n",
       "        -9.46853235e-02,  -1.62479088e-01,  -5.22749592e-03,\n",
       "         2.01477483e-02,   6.52860999e-02,   1.38510475e-02,\n",
       "        -7.15491474e-02,   1.18419744e-01,   9.92804766e-02,\n",
       "         6.90661147e-02,   4.37802412e-02,  -1.07853979e-01,\n",
       "         2.64337152e-01,   3.57694067e-02,  -5.46093397e-02,\n",
       "        -3.88266891e-02,   1.76152721e-01,   4.47880104e-02,\n",
       "         1.41123505e-02,   1.72312647e-01,  -1.89423859e-01,\n",
       "         8.91256407e-02,   7.03314319e-02,  -9.15596187e-02,\n",
       "        -2.83403751e-02,  -1.59109265e-01,   4.99096625e-02,\n",
       "        -8.87983814e-02,  -9.35059041e-02,  -1.50791392e-01,\n",
       "        -2.26725023e-02], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['money']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-29 12:02:37,417:INFO:precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('used', 0.9999058842658997),\n",
       " ('wouldn', 0.9999038577079773),\n",
       " ('either', 0.9999014139175415),\n",
       " ('which', 0.9998998641967773),\n",
       " ('though', 0.9998998045921326),\n",
       " ('around', 0.9998981952667236),\n",
       " ('man', 0.9998942613601685),\n",
       " ('men', 0.9998933672904968),\n",
       " ('without', 0.9998931884765625),\n",
       " ('might', 0.9998892545700073)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('women')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99967636498183299"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('money','credit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(inp1, inp2):\n",
    "    return np.dot(inp1, inp2) / (np.linalg.norm(inp1)*np.linalg.norm(inp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_similarity(text1, text2):\n",
    "    # Lower and tokenize the words\n",
    "    text1 = text1.lower().split()\n",
    "    text2 = text2.lower().split()\n",
    "    \n",
    "    # Get a list of word vectors for each word in the sentence\n",
    "    vector1 = np.array([model[word] for word in text1])\n",
    "    vector2 = np.array([model[word] for word in text2])\n",
    "    avg1_vector1 = np.mean(vector1,axis =0)\n",
    "    avg1_vector2 = np.mean(vector2,axis =0)\n",
    "    return cosine_similarity(avg1_vector1,avg1_vector2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
