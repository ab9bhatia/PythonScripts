{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ab9bh\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable as Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from scripts.preprocess import *\n",
    "from sklearn.metrics import f1_score\n",
    "from gensim.models import Word2Vec as word2vec\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A dict of some additional special words\n",
    "X_WORDS = {\"unknown\": \"<unk>\", \"start\": \"<start>\", \"end\": \"<end>\", \"digit\": \"<digit>\"}\n",
    "\n",
    "\n",
    "def add_boundary_tags(tokens):\n",
    "    \"\"\"\n",
    "    Adds start and end tags to list of tokens\n",
    "    \n",
    "    :param tokens: list: list of tokenized words\n",
    "    :returns str: [<start>, w1, w2...., wn, <end>]\n",
    "    \"\"\"\n",
    "    return [X_WORDS[\"start\"]] + tokens + [X_WORDS[\"end\"]]\n",
    "\n",
    "\n",
    "def preprocess(documents, to_lower=True, boundary_tags=False):\n",
    "    \"\"\"\n",
    "    Preprocesses raw text - convert into lowercase add boundary tags\n",
    "    \n",
    "    :param documents: list: of str\n",
    "    :param to_lower: bool: whether to convert text into lowercase(default=True)\n",
    "    :param boundary_tags: bool: whether to keep boundary tags or not(start, end)\n",
    "    :returns processed: list: of list: of str: a list of lists of words\n",
    "    \"\"\"\n",
    "    processed = list() \n",
    "    \n",
    "    for doc in documents:\n",
    "        \n",
    "        # Convert into lowercase if flag is set\n",
    "        if to_lower:\n",
    "            doc = doc.lower()\n",
    "        tokens = [c for c in doc]\n",
    "        if boundary_tags:\n",
    "            tokens = add_boundary_tags(tokens)\n",
    "        processed.append(tokens)\n",
    "        \n",
    "    return processed\n",
    "\n",
    "\n",
    "def to_indices(document, to_ix):\n",
    "    \"\"\"\n",
    "    Converts documents into a list of indices.\n",
    "    \n",
    "    :param documents: list: of list: of str: a list of lists of words\n",
    "    :param to_ix: dict: a word to index mapping\n",
    "    :returns indices: list: of list: of int: a list of lists of word indices\n",
    "    \"\"\"    \n",
    "    indices = list()\n",
    "        \n",
    "    for word in document:\n",
    "        try:\n",
    "            # Look for the word in dict\n",
    "            indices.append(to_ix[word])\n",
    "        except:\n",
    "            # If not found then add a special word for unknown\n",
    "            indices.append(to_ix[X_WORDS[\"unknown\"]])\n",
    "        \n",
    "    return indices\n",
    "\n",
    "\n",
    "def w2v_word_mapping(model_path):\n",
    "    \"\"\"\n",
    "    Returns mapping of words to indices and vice-versa.\n",
    "    In addition to a numpy matrix representation of\n",
    "    pre-trained word vectors with gensim.\n",
    "    \n",
    "    :param model_path: str: Relative path to the pre-trained gensim model    \n",
    "    :returns (word_vectors: np.array: of float: A matrix representation of gensim word vectors,\n",
    "              index_to_word: list: Index to word mapping,\n",
    "              word_to_index: dict: Word to Index mapping)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load Word Vector Model and get a list of vocab\n",
    "    wv_model = word2vec.load(model_path)\n",
    "    index_to_word = list(wv_model.wv.vocab.keys())\n",
    "   \n",
    "    word_vectors = list()\n",
    "    \n",
    "    # Populate matrix of word vectors\n",
    "    for word in index_to_word:\n",
    "        word_vectors.append(wv_model[word])\n",
    "    \n",
    "    # Add a special words(unknow, start, end)\n",
    "    index_to_word += X_WORDS.values()\n",
    "    \n",
    "    # Create a reverse mapping for words\n",
    "    word_to_index = dict((word, idx) for idx, word in enumerate(index_to_word))    \n",
    "    \n",
    "    for word in X_WORDS:\n",
    "        # A random_vector for special words\n",
    "        random_vector = np.random.rand(wv_model.vector_size)\n",
    "        word_vectors.append(random_vector)\n",
    "    \n",
    "    return np.array(word_vectors), index_to_word, word_to_index\n",
    "\n",
    "\n",
    "def get_word_mappings(documents):\n",
    "    \"\"\"\n",
    "    Returns unique words in a list of strings\n",
    "    \n",
    "    :param documents: list: a list of lists    \n",
    "    :returns (None, index_to_word: list: Index to word mapping,\n",
    "              word_to_index: dict: Word to Index mapping)\n",
    "    \"\"\"\n",
    "    \n",
    "    # If type of documents is a list of words then join them together\n",
    "    if type(documents[0]) == list:\n",
    "        documents = [\" \".join(doc) for doc in documents]\n",
    "        \n",
    "    vocab = (\" \".join(documents).split()) + [X_WORDS[\"unknown\"]] # End tags will already be there\n",
    "    index_to_word = np.unique(vocab)\n",
    "    word_to_index = dict((word, idx) for idx, word in enumerate(index_to_word))\n",
    "    \n",
    "    return None, index_to_word, word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"data/penn/train.txt\"\n",
    "TEST_FILE = \"data/penn/test.txt\"\n",
    "\n",
    "train_data = preprocess(open(TRAIN_FILE, 'r').readlines(), boundary_tags=True)\n",
    "test_data = preprocess(open(TEST_FILE, 'r').readlines(), boundary_tags=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start>',\n",
       " ' ',\n",
       " 'p',\n",
       " 'l',\n",
       " 'a',\n",
       " 'n',\n",
       " 's',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'a',\n",
       " 't',\n",
       " ' ',\n",
       " 'g',\n",
       " 'i',\n",
       " 'v',\n",
       " 'e',\n",
       " ' ',\n",
       " 'a',\n",
       " 'd',\n",
       " 'v',\n",
       " 'e',\n",
       " 'r',\n",
       " 't',\n",
       " 'i',\n",
       " 's',\n",
       " 'e',\n",
       " 'r',\n",
       " 's',\n",
       " ' ',\n",
       " 'd',\n",
       " 'i',\n",
       " 's',\n",
       " 'c',\n",
       " 'o',\n",
       " 'u',\n",
       " 'n',\n",
       " 't',\n",
       " 's',\n",
       " ' ',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " ' ',\n",
       " 'm',\n",
       " 'a',\n",
       " 'i',\n",
       " 'n',\n",
       " 't',\n",
       " 'a',\n",
       " 'i',\n",
       " 'n',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'o',\n",
       " 'r',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " 'c',\n",
       " 'r',\n",
       " 'e',\n",
       " 'a',\n",
       " 's',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'a',\n",
       " 'd',\n",
       " ' ',\n",
       " 's',\n",
       " 'p',\n",
       " 'e',\n",
       " 'n',\n",
       " 'd',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'h',\n",
       " 'a',\n",
       " 'v',\n",
       " 'e',\n",
       " ' ',\n",
       " 'b',\n",
       " 'e',\n",
       " 'c',\n",
       " 'o',\n",
       " 'm',\n",
       " 'e',\n",
       " ' ',\n",
       " 'p',\n",
       " 'e',\n",
       " 'r',\n",
       " 'm',\n",
       " 'a',\n",
       " 'n',\n",
       " 'e',\n",
       " 'n',\n",
       " 't',\n",
       " ' ',\n",
       " '<',\n",
       " 'u',\n",
       " 'n',\n",
       " 'k',\n",
       " '>',\n",
       " ' ',\n",
       " 'a',\n",
       " 't',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'n',\n",
       " 'e',\n",
       " 'w',\n",
       " 's',\n",
       " ' ',\n",
       " '<',\n",
       " 'u',\n",
       " 'n',\n",
       " 'k',\n",
       " '>',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " ' ',\n",
       " 'u',\n",
       " 'n',\n",
       " 'd',\n",
       " 'e',\n",
       " 'r',\n",
       " 's',\n",
       " 'c',\n",
       " 'o',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'f',\n",
       " 'i',\n",
       " 'e',\n",
       " 'r',\n",
       " 'c',\n",
       " 'e',\n",
       " ' ',\n",
       " 'c',\n",
       " 'o',\n",
       " 'm',\n",
       " 'p',\n",
       " 'e',\n",
       " 't',\n",
       " 'i',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 'b',\n",
       " 'e',\n",
       " 't',\n",
       " 'w',\n",
       " 'e',\n",
       " 'e',\n",
       " 'n',\n",
       " ' ',\n",
       " 'n',\n",
       " 'e',\n",
       " 'w',\n",
       " 's',\n",
       " 'w',\n",
       " 'e',\n",
       " 'e',\n",
       " 'k',\n",
       " ' ',\n",
       " 't',\n",
       " 'i',\n",
       " 'm',\n",
       " 'e',\n",
       " ' ',\n",
       " 'w',\n",
       " 'a',\n",
       " 'r',\n",
       " 'n',\n",
       " 'e',\n",
       " 'r',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " 'c',\n",
       " '.',\n",
       " ' ',\n",
       " \"'\",\n",
       " 's',\n",
       " ' ',\n",
       " 't',\n",
       " 'i',\n",
       " 'm',\n",
       " 'e',\n",
       " ' ',\n",
       " 'm',\n",
       " 'a',\n",
       " 'g',\n",
       " 'a',\n",
       " 'z',\n",
       " 'i',\n",
       " 'n',\n",
       " 'e',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " ' ',\n",
       " '<',\n",
       " 'u',\n",
       " 'n',\n",
       " 'k',\n",
       " '>',\n",
       " ' ',\n",
       " 'b',\n",
       " '.',\n",
       " ' ',\n",
       " '<',\n",
       " 'u',\n",
       " 'n',\n",
       " 'k',\n",
       " '>',\n",
       " ' ',\n",
       " \"'\",\n",
       " 's',\n",
       " ' ',\n",
       " 'u',\n",
       " '.',\n",
       " 's',\n",
       " '.',\n",
       " ' ',\n",
       " 'n',\n",
       " 'e',\n",
       " 'w',\n",
       " 's',\n",
       " ' ',\n",
       " '&',\n",
       " ' ',\n",
       " 'w',\n",
       " 'o',\n",
       " 'r',\n",
       " 'l',\n",
       " 'd',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " 'p',\n",
       " 'o',\n",
       " 'r',\n",
       " 't',\n",
       " ' ',\n",
       " '\\n',\n",
       " '<end>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get word vectors and indices mappings\n",
    "WORD_VECTORS, INDEX_TO_WORD, WORD_TO_INDEX = get_word_mappings(documents=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = [(sample[:-1], sample[1:]) for sample in train_data]\n",
    "test_data = [(sample[:-1], sample[1:]) for sample in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA = [(to_indices(x, WORD_TO_INDEX),\n",
    "               to_indices(y, WORD_TO_INDEX)) for x, y in train_data]\n",
    "TEST_DATA = [(to_indices(x, WORD_TO_INDEX),\n",
    "               to_indices(y, WORD_TO_INDEX)) for x, y in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, index_to_tag, embedding_dim=None, vocab_size=None,\n",
    "                 embeddings=None, dropout=0.5, output_activation=\"tanh\", num_layers=1,\n",
    "                 batch_size=1, bidirectional=False, classifier=True, has_cuda=True):\n",
    "        \"\"\"\n",
    "        LSTM Classifier performs multi class classification and Sequence Tagging.\n",
    "        \n",
    "        :param hidden_dim: int: Number of hidden layers in the LSTM\n",
    "        :param tag_to_index: dict: Mapping of output labels with indices\n",
    "        :param embedding_dim: int: Word embeddings dimension        \n",
    "        :param vocab_size: int: Number of unique words in the dataset\n",
    "        :param embeddings: numpy.matrix: Pre-trained word words\n",
    "        :param dropout: float: dropout value\n",
    "        :param output_activation: str: one of the values from [\"tanh\", \"relu\", \"sigmoid\"]\n",
    "        :param num_layers: int: Number of LSTM layers\n",
    "        :param batch_size: int: Number of samples in one batch\n",
    "        :param bidirectional: bool: Bidirectional LSTM or not\n",
    "        :param classsifier: bool: Is this model a classifier(include softmax)\n",
    "        :param has_cuda: bool: Whether to run this model on gpu or not\n",
    "        \"\"\"        \n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        activations = {\"tanh\": F.tanh, \"relu\": F.relu, \"sigmoid\": F.sigmoid, \"\": False}\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.has_cuda = has_cuda\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.index_to_tag = index_to_tag\n",
    "        self.classifier = classifier\n",
    "        self.output_activation = activations[output_activation]\n",
    "        num_labels = len(index_to_tag)\n",
    "        \n",
    "        # If directional set directions to 2\n",
    "        self.directions = 1\n",
    "        if bidirectional:\n",
    "            self.directions = 2\n",
    "        \n",
    "        # Setup embeddings\n",
    "        if not embeddings is None:\n",
    "            embedding_dim = embeddings.shape[1]\n",
    "            self.word_embeddings = nn.Embedding(*embeddings.shape)\n",
    "            self.word_embeddings.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "        elif embedding_dim and vocab_size:\n",
    "            self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        else:\n",
    "            print(\"You must provide either a pre-trained word vectors matrix as\\\n",
    "                  'embeddings' or 'embedding_dim' and 'vocab_size'\")\n",
    "            return None\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, dropout=dropout,\n",
    "                            num_layers=num_layers, batch_first=False,\n",
    "                            bidirectional=bidirectional) # Coz I like my batch first ;)\n",
    "        self.h2o = nn.Linear(hidden_dim*self.directions, num_labels) # Concatenates if bi-directional\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        \"\"\"\n",
    "        Initialize the hidden states for LSTM\n",
    "        \n",
    "        :returns : tuple: of (autograd.Variable, autograd.Variable)\n",
    "        \"\"\"\n",
    "        if self.has_cuda:\n",
    "            return (Variable(torch.zeros(self.num_layers*self.directions,\n",
    "                                                  self.batch_size,\n",
    "                                                  self.hidden_dim).cuda()),\n",
    "                    Variable(torch.zeros(self.num_layers*self.directions,\n",
    "                                                  self.batch_size,\n",
    "                                                  self.hidden_dim).cuda()))\n",
    "        else:\n",
    "            return (Variable(torch.zeros(self.num_layers*self.directions,\n",
    "                                                  self.batch_size,\n",
    "                                                  self.hidden_dim)),\n",
    "                    Variable(torch.zeros(self.num_layers*self.directions,\n",
    "                                                  self.batch_size,\n",
    "                                                  self.hidden_dim)))\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"\n",
    "        Forward-Pass for RNN, which returns the probability scores of classes. \n",
    "        \n",
    "        :param tokens: autograd.Variable: a list of indices as torch tensors\n",
    "        \n",
    "        :returns: scores: autograd.Variable: Final score for the model\n",
    "        \"\"\"\n",
    "        embeds = self.drop(self.word_embeddings(tokens))\n",
    "        self.output, self.hidden = self.lstm(embeds.view(len(tokens), 1, -1), self.hidden)\n",
    "        self.hidden = (self.hidden[0].detach(), self.hidden[1].detach())\n",
    "        \n",
    "        final_output = self.h2o(F.tanh(self.drop(self.output.view(len(tokens), -1))))\n",
    "        scores = F.log_softmax(final_output)\n",
    "       \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_cuda():\n",
    "    return True if torch.cuda.is_available() else False\n",
    "\n",
    "CUDA = is_cuda()\n",
    "\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(1234)\n",
    "else:\n",
    "    torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_Variable(sequence, has_cuda=is_cuda(), ttype=torch.LongTensor):\n",
    "    \"\"\"\n",
    "    Convert a list of words to list of pytorch tensor variables\n",
    "    \n",
    "    :param tokens: list: of str: a list of words in a sentence\n",
    "    :param has_cuda: bool: does this machine has cuda\n",
    "    :param ttype: torch tensor type\n",
    "    :returns : autograd.Variable\n",
    "    \"\"\"\n",
    "    if has_cuda:\n",
    "        tensor = ttype(sequence).cuda()\n",
    "    else:\n",
    "        tensor = ttype(sequence)\n",
    "        \n",
    "    return Variable(tensor)\n",
    "\n",
    "\n",
    "def get_accuracy(x, y):\n",
    "    \"\"\"\n",
    "    Calculates percent of similar instances among two numpy arrays\n",
    "    \n",
    "    :param x: np.array\n",
    "    :param y: np.array\n",
    "    \n",
    "    :returns accuracy: float\n",
    "    \"\"\"\n",
    "    accuracy = np.sum(x == y)/len(x)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def get_metrics(x, y, num_labels):\n",
    "    \"\"\"\n",
    "    Get F1 Score and accuracy for a predicted and target values.\n",
    "    \n",
    "    :param x: np.array\n",
    "    :param y: np.array\n",
    "    :param num_labels: number of unique labels in dataset\n",
    "    :returns (total_f1_score: float, total_accuracy: float)\n",
    "    \"\"\"    \n",
    "    total_f1_score = 0\n",
    "    total_accuracy = 0\n",
    "    \n",
    "    for inp, out in zip(x, y):        \n",
    "        f1 = f1_score(inp, list(out), labels=np.arange(num_labels), average='macro')\n",
    "        \n",
    "        total_f1_score += f1\n",
    "        total_accuracy += get_accuracy(inp, out)        \n",
    "        \n",
    "    return total_f1_score/len(x), total_accuracy/len(x)\n",
    "\n",
    "\n",
    "def predict(model, x):\n",
    "    \"\"\"\n",
    "    Get the prediction as the class name from trained model.\n",
    "    \n",
    "    :param model: pytorch model\n",
    "    :param x: str: a test document\n",
    "    \n",
    "    :returns tag: int: class id for the input\n",
    "    \"\"\"\n",
    "    # Set model to evalution state to turn off dropout\n",
    "    model.eval()\n",
    "    x = to_Variable(x)\n",
    "    yhat = model(x)\n",
    "    _, tag = yhat.max(1)\n",
    "    \n",
    "    return tag.data.cpu().numpy()\n",
    "\n",
    "\n",
    "def evaluate(model, eval_data, num_labels):\n",
    "    \"\"\"\n",
    "    Evaluates the accuracy for the model in the global scope.\n",
    "    \n",
    "    :param model: PyTorch Model\n",
    "    :param eval_data: tuple: as (inputs, targets)\n",
    "    :param num_labels: number of unique labels in dataset\n",
    "    :returns (f1_score: float, accuracy: float)\n",
    "    \"\"\"    \n",
    "    # Turn on the evaluation state to ignore dropouts\n",
    "    model.eval()\n",
    "    results = [predict(model, x) for x, y in eval_data]\n",
    "    f1_score, accuracy = get_metrics(np.array([y for x, y in eval_data]), results, num_labels)\n",
    "    return f1_score, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hparams = {'hidden_dim': 128, 'learning_rate': 0.05, \n",
    "           'epochs': 10, 'dropout': 0.5, 'embedding_dim': 8,\n",
    "           'vocab_size': len(INDEX_TO_WORD), 'clip_value': 0.5}\n",
    "\n",
    "model = LSTM(hidden_dim=hparams['hidden_dim'], index_to_tag=INDEX_TO_WORD, \n",
    "             embedding_dim=hparams['embedding_dim'], bidirectional=True,\n",
    "             vocab_size=hparams['vocab_size'], dropout=hparams['dropout'])\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer =  optim.SGD(model.parameters(), lr=hparams['learning_rate'],\n",
    "                                          weight_decay=0.0001, momentum=0.9)\n",
    "\n",
    "if CUDA:\n",
    "    model = model.cuda()\n",
    "    loss_fn = loss_fn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ab9bh\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 - Average Loss after 1000 samples: 1.602665\n",
      "Epoch  0 - Average Loss after 2000 samples: 0.627919\n",
      "Epoch  0 - Average Loss after 3000 samples: 0.442929\n",
      "Epoch  0 - Average Loss after 4000 samples: 0.354921\n",
      "Epoch  0 - Average Loss after 5000 samples: 0.304963\n",
      "Epoch  0 - Average Loss after 6000 samples: 0.263634\n",
      "Epoch  0 - Average Loss after 7000 samples: 0.241821\n",
      "Epoch  0 - Average Loss after 8000 samples: 0.222775\n",
      "Epoch  0 - Average Loss after 9000 samples: 0.210082\n",
      "Epoch  0 - Average Loss after 10000 samples: 0.196932\n",
      "Epoch  0 - Average Loss after 11000 samples: 0.182569\n",
      "Epoch  0 - Average Loss after 12000 samples: 0.173495\n",
      "Epoch  0 - Average Loss after 13000 samples: 0.167925\n",
      "Epoch  0 - Average Loss after 14000 samples: 0.162792\n",
      "Epoch  0 - Average Loss after 15000 samples: 0.158948\n",
      "Epoch  0 - Average Loss after 16000 samples: 0.155340\n",
      "Epoch  0 - Average Loss after 17000 samples: 0.150301\n",
      "Epoch  0 - Average Loss after 18000 samples: 0.147705\n",
      "Epoch  0 - Average Loss after 19000 samples: 0.141878\n",
      "Epoch  0 - Average Loss after 20000 samples: 0.143914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ab9bh\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\ab9bh\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 - Train F1 Score, Accuracy after 20000 samples: 0.451806, 0.986901\n",
      "Epoch  0 - Test F1 Score, Accuracy after 20000 samples: 0.451238, 0.986839\n",
      "Epoch  0 - Average Loss after 21000 samples: 0.140990\n",
      "Epoch  0 - Average Loss after 22000 samples: 0.137399\n",
      "Epoch  0 - Average Loss after 23000 samples: 0.137394\n",
      "Epoch  0 - Average Loss after 24000 samples: 0.140888\n",
      "Epoch  0 - Average Loss after 25000 samples: 0.130640\n",
      "Epoch  0 - Average Loss after 26000 samples: 0.130249\n",
      "Epoch  0 - Average Loss after 27000 samples: 0.128707\n",
      "Epoch  0 - Average Loss after 28000 samples: 0.131580\n",
      "Epoch  0 - Average Loss after 29000 samples: 0.126603\n",
      "Epoch  0 - Average Loss after 30000 samples: 0.129997\n",
      "Epoch  0 - Average Loss after 31000 samples: 0.122367\n",
      "Epoch  0 - Average Loss after 32000 samples: 0.125670\n",
      "Epoch  0 - Average Loss after 33000 samples: 0.124513\n",
      "Epoch  0 - Average Loss after 34000 samples: 0.121786\n",
      "Epoch  0 - Average Loss after 35000 samples: 0.127029\n",
      "Epoch  0 - Average Loss after 36000 samples: 0.123592\n",
      "Epoch  0 - Average Loss after 37000 samples: 0.118372\n",
      "Epoch  0 - Average Loss after 38000 samples: 0.120966\n",
      "Epoch  0 - Average Loss after 39000 samples: 0.112575\n",
      "Epoch  0 - Average Loss after 40000 samples: 0.117867\n",
      "Epoch  0 - Train F1 Score, Accuracy after 40000 samples: 0.451226, 0.984877\n",
      "Epoch  0 - Test F1 Score, Accuracy after 40000 samples: 0.450134, 0.984428\n",
      "Epoch  0 - Average Loss after 41000 samples: 0.123498\n",
      "Epoch  0 - Average Loss after 42000 samples: 0.116153\n",
      "AVERAGE EPOCH LOSS and PERPLEXITY: (0.21038868, 1.2341577)\n",
      "Epoch  1 - Average Loss after 1000 samples: 0.119620\n",
      "Epoch  1 - Average Loss after 2000 samples: 0.113982\n",
      "Epoch  1 - Average Loss after 3000 samples: 0.116535\n",
      "Epoch  1 - Average Loss after 4000 samples: 0.119500\n",
      "Epoch  1 - Average Loss after 5000 samples: 0.115454\n",
      "Epoch  1 - Average Loss after 6000 samples: 0.112179\n",
      "Epoch  1 - Average Loss after 7000 samples: 0.115497\n",
      "Epoch  1 - Average Loss after 8000 samples: 0.111728\n",
      "Epoch  1 - Average Loss after 9000 samples: 0.117193\n",
      "Epoch  1 - Average Loss after 10000 samples: 0.112073\n",
      "Epoch  1 - Average Loss after 11000 samples: 0.110658\n",
      "Epoch  1 - Average Loss after 12000 samples: 0.103851\n",
      "Epoch  1 - Average Loss after 13000 samples: 0.116400\n",
      "Epoch  1 - Average Loss after 14000 samples: 0.117239\n",
      "Epoch  1 - Average Loss after 15000 samples: 0.107199\n",
      "Epoch  1 - Average Loss after 16000 samples: 0.110353\n",
      "Epoch  1 - Average Loss after 17000 samples: 0.109695\n",
      "Epoch  1 - Average Loss after 18000 samples: 0.111868\n",
      "Epoch  1 - Average Loss after 19000 samples: 0.109109\n",
      "Epoch  1 - Average Loss after 20000 samples: 0.108182\n",
      "Epoch  1 - Train F1 Score, Accuracy after 20000 samples: 0.452220, 0.986206\n",
      "Epoch  1 - Test F1 Score, Accuracy after 20000 samples: 0.453135, 0.986424\n",
      "Epoch  1 - Average Loss after 21000 samples: 0.102926\n",
      "Epoch  1 - Average Loss after 22000 samples: 0.108941\n",
      "Epoch  1 - Average Loss after 23000 samples: 0.108763\n",
      "Epoch  1 - Average Loss after 24000 samples: 0.109228\n",
      "Epoch  1 - Average Loss after 25000 samples: 0.107059\n",
      "Epoch  1 - Average Loss after 26000 samples: 0.110407\n",
      "Epoch  1 - Average Loss after 27000 samples: 0.107963\n",
      "Epoch  1 - Average Loss after 28000 samples: 0.104874\n",
      "Epoch  1 - Average Loss after 29000 samples: 0.106620\n",
      "Epoch  1 - Average Loss after 30000 samples: 0.104703\n",
      "Epoch  1 - Average Loss after 31000 samples: 0.110333\n",
      "Epoch  1 - Average Loss after 32000 samples: 0.108838\n",
      "Epoch  1 - Average Loss after 33000 samples: 0.112762\n",
      "Epoch  1 - Average Loss after 34000 samples: 0.103468\n",
      "Epoch  1 - Average Loss after 35000 samples: 0.101366\n",
      "Epoch  1 - Average Loss after 36000 samples: 0.108605\n",
      "Epoch  1 - Average Loss after 37000 samples: 0.103917\n",
      "Epoch  1 - Average Loss after 38000 samples: 0.102272\n",
      "Epoch  1 - Average Loss after 39000 samples: 0.103538\n",
      "Epoch  1 - Average Loss after 40000 samples: 0.104711\n",
      "Epoch  1 - Train F1 Score, Accuracy after 40000 samples: 0.452588, 0.984367\n",
      "Epoch  1 - Test F1 Score, Accuracy after 40000 samples: 0.453267, 0.984505\n",
      "Epoch  1 - Average Loss after 41000 samples: 0.102263\n",
      "Epoch  1 - Average Loss after 42000 samples: 0.106179\n",
      "AVERAGE EPOCH LOSS and PERPLEXITY: (0.10946278, 1.1156785)\n",
      "Epoch  2 - Average Loss after 1000 samples: 0.104999\n",
      "Epoch  2 - Average Loss after 2000 samples: 0.104633\n",
      "Epoch  2 - Average Loss after 3000 samples: 0.101641\n",
      "Epoch  2 - Average Loss after 4000 samples: 0.104905\n",
      "Epoch  2 - Average Loss after 5000 samples: 0.100457\n",
      "Epoch  2 - Average Loss after 6000 samples: 0.104869\n",
      "Epoch  2 - Average Loss after 7000 samples: 0.097988\n",
      "Epoch  2 - Average Loss after 8000 samples: 0.104641\n",
      "Epoch  2 - Average Loss after 9000 samples: 0.101871\n",
      "Epoch  2 - Average Loss after 10000 samples: 0.109282\n",
      "Epoch  2 - Average Loss after 11000 samples: 0.105886\n",
      "Epoch  2 - Average Loss after 12000 samples: 0.101437\n",
      "Epoch  2 - Average Loss after 13000 samples: 0.095579\n",
      "Epoch  2 - Average Loss after 14000 samples: 0.100270\n",
      "Epoch  2 - Average Loss after 15000 samples: 0.097287\n",
      "Epoch  2 - Average Loss after 16000 samples: 0.098049\n",
      "Epoch  2 - Average Loss after 17000 samples: 0.101136\n",
      "Epoch  2 - Average Loss after 18000 samples: 0.097400\n",
      "Epoch  2 - Average Loss after 19000 samples: 0.102424\n",
      "Epoch  2 - Average Loss after 20000 samples: 0.107264\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-6bfad521b6b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mtest_after\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             train_f1_score, train_accuracy = evaluate(model, TRAIN_DATA[:len(TEST_DATA)],\n\u001b[1;32m---> 46\u001b[1;33m                                                                         len(WORD_TO_INDEX))\n\u001b[0m\u001b[0;32m     47\u001b[0m             print(\"Epoch % d - Train F1 Score, Accuracy after %d samples: %f, %f\"% (epoch,\n\u001b[0;32m     48\u001b[0m                                                                                         \u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-88bfbab38446>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(model, eval_data, num_labels)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;31m# Turn on the evaluation state to ignore dropouts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meval_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meval_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-88bfbab38446>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;31m# Turn on the evaluation state to ignore dropouts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meval_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meval_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-88bfbab38446>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(model, x)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_Variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-92af9a101448>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \"\"\"\n\u001b[0;32m     87\u001b[0m         \u001b[0membeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         )\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhack_onnx_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\function.py\u001b[0m in \u001b[0;36m_do_forward\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[0mflat_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_iter_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m         \u001b[0mflat_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNestedIOFunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mflat_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m         \u001b[0mnested_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[0mnested_variables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\function.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[0mnested_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_map_variable_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_extended\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward_extended\u001b[1;34m(self, input, weight, hx)\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[0mhy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m         \u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\backends\\cudnn\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(fn, input, hx, weight, output, hy)\u001b[0m\n\u001b[0;32m    303\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcy_desc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m                 \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m                 \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreserve\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreserve\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m             ))\n\u001b[0;32m    307\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# inference\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_after = 1000\n",
    "test_after = 20000\n",
    "\n",
    "for epoch in range(hparams['epochs']):\n",
    "\n",
    "    count = 0\n",
    "    avg_loss = 0\n",
    "    epoch_loss = 0\n",
    "    test_f1_score = 0\n",
    "    last_test_f1_score = 0\n",
    "\n",
    "    # Randomly shuffle the dataset\n",
    "    np.random.shuffle(TRAIN_DATA)\n",
    "    np.random.shuffle(TEST_DATA)\n",
    "\n",
    "    for tokens, labels in TRAIN_DATA:\n",
    "\n",
    "        x, y = to_Variable(tokens), to_Variable(labels)        \n",
    "\n",
    "        y_ = model(x)        \n",
    "        loss = loss_fn(y_, y)\n",
    "\n",
    "        # Initialize hidden states to zero\n",
    "        model.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm(model.parameters(), hparams['clip_value'])\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-hparams['learning_rate'], p.grad.data)\n",
    "\n",
    "        loss_value = loss.data.cpu().numpy()\n",
    "        avg_loss += loss_value\n",
    "        epoch_loss += loss_value\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        if count%print_after == 0:\n",
    "            print(\"Epoch % d - Average Loss after %d samples: %f\" % (epoch, count,\n",
    "                                                                     avg_loss/print_after))\n",
    "            avg_loss = 0\n",
    "\n",
    "        if count%test_after == 0:\n",
    "            train_f1_score, train_accuracy = evaluate(model, TRAIN_DATA[:len(TEST_DATA)],\n",
    "                                                                        len(WORD_TO_INDEX))\n",
    "            print(\"Epoch % d - Train F1 Score, Accuracy after %d samples: %f, %f\"% (epoch,\n",
    "                                                                                        count,\n",
    "                                                                                        train_f1_score,\n",
    "                                                                                        train_accuracy))\n",
    "\n",
    "            test_f1_score, test_accuracy = evaluate(model, TEST_DATA,\n",
    "                                                    len(WORD_TO_INDEX)) # So that we can use it later\n",
    "            print(\"Epoch % d - Test F1 Score, Accuracy after %d samples: %f, %f\" % (epoch,\n",
    "                                                                                       count,\n",
    "                                                                                       test_f1_score,\n",
    "                                                                                       test_accuracy))\n",
    "            model.train() # Get the model back to training state\n",
    "    \n",
    "    l = (epoch_loss/len(TRAIN_DATA))[0]\n",
    "    print(\"AVERAGE EPOCH LOSS and PERPLEXITY:\", (l, np.exp(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "e\n",
      "v\n",
      "e\n",
      "n\n",
      "<unk>\n",
      "l\n",
      "e\n",
      "t\n",
      "e\n",
      "r\n",
      "<unk>\n",
      "l\n",
      "y\n",
      "n\n",
      "c\n",
      "h\n",
      "<unk>\n",
      "m\n",
      "a\n",
      "n\n",
      "m\n",
      "g\n",
      "e\n",
      "r\n",
      "<unk>\n",
      "o\n",
      "f\n",
      "<unk>\n",
      "f\n",
      "i\n",
      "d\n",
      "e\n",
      "l\n",
      "i\n",
      "t\n",
      "y\n",
      "<unk>\n",
      "'\n",
      "s\n",
      "<unk>\n",
      "$\n",
      "<unk>\n",
      "n\n",
      "<unk>\n",
      "b\n",
      "i\n",
      "l\n",
      "l\n",
      "i\n",
      "o\n",
      "n\n",
      "<unk>\n",
      "<\n",
      "u\n",
      "n\n",
      "k\n",
      ">\n",
      "<unk>\n",
      "f\n",
      "u\n",
      "n\n",
      "d\n",
      "<unk>\n",
      "t\n",
      "h\n",
      "e\n",
      "<unk>\n",
      "n\n",
      "m\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "<unk>\n",
      "'\n",
      "s\n",
      "<unk>\n",
      "l\n",
      "a\n",
      "r\n",
      "g\n",
      "e\n",
      "s\n",
      "t\n",
      "<unk>\n",
      "s\n",
      "t\n",
      "o\n",
      "c\n",
      "k\n",
      "<unk>\n",
      "f\n",
      "u\n",
      "n\n",
      "d\n",
      "<unk>\n",
      "b\n",
      "u\n",
      "i\n",
      "l\n",
      "t\n",
      "<unk>\n",
      "u\n",
      "l\n",
      "<unk>\n",
      "c\n",
      "a\n",
      "s\n",
      "h\n",
      "<unk>\n",
      "t\n",
      "o\n",
      "<unk>\n",
      "n\n",
      "<unk>\n",
      "n\n",
      "<unk>\n",
      "o\n",
      "r\n",
      "<unk>\n",
      "$\n",
      "<unk>\n",
      "n\n",
      "<unk>\n",
      "m\n",
      "i\n",
      "l\n",
      "l\n",
      "i\n",
      "o\n",
      "n\n",
      "<unk>\n",
      "<unk>\n",
      "<end>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ab9bh\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "for word in predict(model, TEST_DATA[10][0]):\n",
    "    print(INDEX_TO_WORD[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "e\n",
      "v\n",
      "e\n",
      "n\n",
      "<unk>\n",
      "p\n",
      "e\n",
      "t\n",
      "e\n",
      "r\n",
      "<unk>\n",
      "l\n",
      "y\n",
      "n\n",
      "c\n",
      "h\n",
      "<unk>\n",
      "m\n",
      "a\n",
      "n\n",
      "a\n",
      "g\n",
      "e\n",
      "r\n",
      "<unk>\n",
      "o\n",
      "f\n",
      "<unk>\n",
      "f\n",
      "i\n",
      "d\n",
      "e\n",
      "l\n",
      "i\n",
      "t\n",
      "y\n",
      "<unk>\n",
      "'\n",
      "s\n",
      "<unk>\n",
      "$\n",
      "<unk>\n",
      "n\n",
      "<unk>\n",
      "b\n",
      "i\n",
      "l\n",
      "l\n",
      "i\n",
      "o\n",
      "n\n",
      "<unk>\n",
      "<\n",
      "u\n",
      "n\n",
      "k\n",
      ">\n",
      "<unk>\n",
      "f\n",
      "u\n",
      "n\n",
      "d\n",
      "<unk>\n",
      "t\n",
      "h\n",
      "e\n",
      "<unk>\n",
      "n\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "<unk>\n",
      "'\n",
      "s\n",
      "<unk>\n",
      "l\n",
      "a\n",
      "r\n",
      "g\n",
      "e\n",
      "s\n",
      "t\n",
      "<unk>\n",
      "s\n",
      "t\n",
      "o\n",
      "c\n",
      "k\n",
      "<unk>\n",
      "f\n",
      "u\n",
      "n\n",
      "d\n",
      "<unk>\n",
      "b\n",
      "u\n",
      "i\n",
      "l\n",
      "t\n",
      "<unk>\n",
      "u\n",
      "p\n",
      "<unk>\n",
      "c\n",
      "a\n",
      "s\n",
      "h\n",
      "<unk>\n",
      "t\n",
      "o\n",
      "<unk>\n",
      "n\n",
      "<unk>\n",
      "n\n",
      "<unk>\n",
      "o\n",
      "r\n",
      "<unk>\n",
      "$\n",
      "<unk>\n",
      "n\n",
      "<unk>\n",
      "m\n",
      "i\n",
      "l\n",
      "l\n",
      "i\n",
      "o\n",
      "n\n",
      "<unk>\n",
      "<unk>\n",
      "<end>\n"
     ]
    }
   ],
   "source": [
    "for word in TEST_DATA[10][1]:\n",
    "    print(INDEX_TO_WORD[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temperature = 1e-4\n",
    "ntokens = len(INDEX_TO_WORD)\n",
    "model.hidden = model.init_hidden()\n",
    "inp = Variable(torch.rand(1, 1).mul(ntokens).long(), volatile=True)\n",
    "\n",
    "if CUDA:\n",
    "    inp.data = inp.data.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/\n",
      "\n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/\n",
      "\n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/\n",
      "\n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/\n",
      "\n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ab9bh\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "num_words = 100\n",
    "\n",
    "for i in range(num_words):\n",
    "    output = model(inp)\n",
    "    word_weights = model.output.squeeze().data.div(temperature).exp().cpu()\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    inp.data.fill_(word_idx)\n",
    "    word = INDEX_TO_WORD[word_idx]\n",
    "\n",
    "    print(word + ('\\n' if i % 20 == 19 else ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
